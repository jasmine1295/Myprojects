# -*- coding: utf-8 -*-
"""imdrf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.c578dt1nqtkAvCd5WlRyr7pEd_Ibjvcom/drive/1m-
"""

import tqdm
import meddra
#import progressbar
import os
import pandas as pd
import numpy as np
import nltk
import re
from nltk.corpus import stopwords
from nltk import word_tokenize, FreqDist
import string
import random
from nltk.tokenize import word_tokenize
import nltk
from nltk.stem import WordNetLemmatizer
#from multi_rake import Rake
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import wordnet
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.engine import URL
from snowflake.sqlalchemy import URL

nltk.download('all')



############################################################################################################################################################################################################
############################################################################################################################################################################################################
###########################################################################.        General Code Check                         #############################################################################
############################################################################################################################################################################################################
############################################################################################################################################################################################################
############################################################################################################################################################################################################

a = pd.read_csv("A_Annexure.csv")
b = pd.read_csv("AnnexB.csv")
c = pd.read_csv("AnnexC.csv")
d = pd.read_csv("AnnexD.csv")
e = pd.read_csv("Annex_E.csv")
f = pd.read_csv("AnnexF.csv")
g = pd.read_csv("Annex_G.csv")
code_list = a['Code'].tolist()+b['Code'].tolist()+c['Code'].tolist()+d['Code'].tolist()+e['Code'].tolist()+f['Code'].tolist()
code_list = [x.lower() for x in code_list]

all_col = ['Definition','Level 1 Term','Level 1 Code', 'Level 2 Term','Level 2 Code', 'Level 3 Term','Level 3 Code']
b_col = ['Definition','Level 1 Term']
d_col = ['Definition','Level 1 Term','Level 1 Code', 'Level 2 Term','Level 2 Code']
    
def get_anne_data(cod,medra,org_foi):
  foi = cod
  cod = str(cod).lower()
  med = []
  try:
    if cod.startswith('a'):
      rdf = a[a['Code'].str.lower()==cod][all_col]
    elif cod.startswith('b'):
      rdf = b[b['Code'].str.lower()==cod][b_col]
      rdf['Level 1 Code'] = foi.upper()
      rdf[['Level 2 Term','Level 2 Code', 'Level 3 Term','Level 3 Code']] = "Not Applicable"
    elif cod.startswith('d'):
      rdf = d[d['Code'].str.lower()==cod][d_col]
      rdf[['Level 3 Term','Level 3 Code']] = "Not Applicable"
    elif cod.startswith('c'):
      rdf = c[c['Code'].str.lower()==cod][all_col]
    elif cod.startswith('e'):
      rdf = e[e['Code'].str.lower()==cod][all_col]
    elif cod.startswith('f'):
      rdf = f[f['Code'].str.lower()==cod][all_col]
    rdf['EventDescription'] = org_foi 
    rdf['Score'] = "1.0"
    rdf.insert(0, 'EventDescription', rdf.pop('EventDescription'))
    rdf.insert(2, 'Score', rdf.pop('Score'))
    rdf = rdf.fillna("Not Applicable")
  except:
    return {'error':'code error'}
  try:
    if (medra == 1):# and (cod.startswith('e')):
      for i in e[e['Code'].str.lower()==cod]['Non-IMDRF Code'].values:
        m_code = i.split(":")[1]
        med.append(meddra.meddraLookup(m_code))
      rdf['Meddra'] = med
      rdf = rdf.fillna("Not Applicable")
  except:
    rdf['Meddra'] = [{"Code":"Not Applicable"}]
  return rdf





############################################################################################################################################################################################################
############################################################################################################################################################################################################
###########################################################################.        Annex - A                         ######################################################################################
############################################################################################################################################################################################################
############################################################################################################################################################################################################
############################################################################################################################################################################################################

###### Purification function
stp_words = stopwords.words('english')
stp_words.remove('no')
stp_words.remove('not')

def purificationV1(string1):
    wordnet_lemmatizer = WordNetLemmatizer()
    string1 = str(string1).lower()
    string1 = re.sub('<[^>]+>', '', string1)
    string1 = string1.replace("-",' ')
    string1 = string1.replace("/",' ')   ## Replace "/"

    char_list = [':']
    string1 = [ele for ele in string1.split() if all(ch not in ele for ch in char_list)]
    string1 = ' '.join(string1)

    tokens = word_tokenize(string1)
    stopwords_list = stp_words + list(string.punctuation)
    stopwords_list += ["''", '""', '...', '``',':',';']
    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]
    numeric_word_removed = [re.sub('[0-9]', '', i) for i in stopwords_removed]
    clean_word_1 = []
    for i in numeric_word_removed:
      clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
    clean_word_1 = list(set(clean_word_1))
    return clean_word_1 #' '.join(clean_word_1)




###### Purification version2 function 

# def purificationV2(string1):
#     wordnet_lemmatizer = WordNetLemmatizer()
#     string1 = str(string1).lower()
#     string1 = re.sub('<[^>]+>', '', string1)
#     string1 = string1.replace("-",' ')
#     text_en = (string1)
#     rake = Rake()
#     try:
#       keywords = rake.apply(text_en)
#       imp_sent = []
#       for i in keywords[:10]:
#         imp_sent.append(i[0])
#       filtered_sent = ' '.join(imp_sent)
#     except:
#       print('1')
#       filtered_sent = string1
#     tokens = word_tokenize(filtered_sent)
#     stopwords_list = stp_words + list(string.punctuation)      #stopwords.words('english').remove('no') + list(string.punctuation)
#     stopwords_list += ["''", '""', '...', '``']
#     stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]
#     numeric_word_removed = [re.sub('[0-9]', '', i) for i in stopwords_removed]
#     clean_word_1 = []
#     for i in numeric_word_removed:
#       clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
#     clean_word_1 = list(set(clean_word_1))
#     return clean_word_1 #' '.join(clean_word_1)

##################################.  Synonyms.   ##################################.  
def syn(word):
  synonyms = []
  antonyms = []
    
  for syn in wordnet.synsets(word):
      for l in syn.lemmas():
          synonyms.append(l.name())
          if l.antonyms():
              antonyms.append(l.antonyms()[0].name())
  
  return list(set(synonyms))
#print(set(antonyms))

##################################.  Synonyms. For The List ##################################.  
def syn_list(word_list):
  synonyms = []
  antonyms = []
  full = []
  
  for word in word_list:
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonyms.append(l.name())
            if l.antonyms():
                antonyms.append(l.antonyms()[0].name())
  
    full = full + list(set(synonyms))
  return full


#################################################################################################################################################################################
#################################################################################################################################################################################
###### Rough
def exploding_feedback_data(data,column_name):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file[column_name] = f_file[column_name].apply(expd)
  f_file = f_file.explode(column_name)

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file[column_name] = f_file[column_name].apply(expd)
  f_file = f_file.explode(column_name)
  return f_file


####### Model retraining
def bulk_retrain(feedback_data,existing_model,new_model_name,code_column_name,feedback_text):
  feedback = pd.read_pickle(existing_model)
  f_file = feedback_data     #pd.read_excel(feedback_file)
  #f_file = imdrf_1.exploding_feedback_data_A(f_file)
  f_file = exploding_feedback_data(f_file,code_column_name)

  ##### Aswin added here below
  def feed(code,text):
    feedback.loc[feedback['Code']==code,'text'] = str(feedback[feedback['Code']==code]['text'].to_list()[0])+' '+str(text)
    return "Data Appended"
  ##### Aswin added here ^up

  p = 0
  for f in f_file[[code_column_name,feedback_text]].values:
    if (str(f[0]).strip() != 'nan') and (str(f[1]).strip() != 'nan'):
      print(p,'= ',f[0])
      try:
        feed(str(f[0]).strip(),f[1])
      except:
        pass
    p+=1

  feedback.to_pickle(new_model_name)


#################################################################################################################################################################################
#################################################################################################################################################################################

###### Rough
def exploding_feedback_data_A(data):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex A'] = f_file['IMDRF Annex A'].apply(expd)
  f_file = f_file.explode('IMDRF Annex A')

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex A'] = f_file['IMDRF Annex A'].apply(expd)
  f_file = f_file.explode('IMDRF Annex A')
  return f_file

###################################################################     Predict
def prediction_A(inp,top,model):
  print("-0",model)
  feedback = pd.read_pickle(model)
  print("-0.12")
  ann = pd.read_csv("A_Annexure.csv")
  df = pd.DataFrame(columns=['EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])  
  dflog = pd.DataFrame()  
  wordnet_lemmatizer = WordNetLemmatizer()
  val = []
  f1_l = []
  out_set_l = []
  len_l = []
  code_l = []
  
  #bar = progressbar.ProgressBar(maxval=len(inp),widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])
  #bar.start()
  #ii = 0

  for j in tqdm.tqdm(inp):    #['A lens was torn']:
    val = []
    v1 = []
    df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])

    innn = purificationV1(j) +j.split()#------> Jasmine Added for performance tuning
    syn_inn = syn_list(innn)#------> Jasmine Added for performance tuning
    
    for k in ann.values:
      wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(feedback[feedback['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      wd = ' '.join(purificationV1(wd)) #test
      a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
      #a = [re.sub('[0-9]', '', i) for i in a]       #### Numeric string removal
      a = [x for x in a if not any(c.isdigit() for c in x)]       #### Numeric string removal
      a = ["".join(list(filter(str.isalnum, line))) for line in a]    ####### SPL char removal
      a = [i for i in a if len(i) > 1]
      clean_word_1 = a
      """ # feedback and annexture syno
      clean_word_1 = []
      for i in a:
        clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
      clean_word_1 = list(set(clean_word_1))
      """

      #in_text = purificationV1(j) +j.split() #.split()   # Commented by Me ------> Jasmine Commented for performance tuning
      #clean_word_1 = purificationV1(' '.join(clean_word_1))

      ###### Adding Synonyms
      def_syn = list(set(syn_list(clean_word_1) + clean_word_1))
      #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin   # ------> Jasmine Commented for performance tuning
      foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin#########################################------> Jasmine Added for performance tuning
      def_syn = [name.lower() for name in def_syn]
      foi_syn = [name.lower() for name in foi_syn]
 
      #out_set = set(clean_word_1) & set(in_text)            # commented by Me
      out_set = set(def_syn) & set(foi_syn)            # Added by Me
      f1 = out_set
      out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
      #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

      if len(out_set) >= 1:
        #print('Match')
        #print(in_text,"===",clean_word_1,"===",out_set)
        #print(clean_word_1)
        f1_l.append(f1)
        out_set_l.append(out_set)
        len_l.append(len(out_set))
        code_l.append(k[3])
        ############################# added for test ^
        
        #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
        v1 = [len(out_set),j,k[3],k[4],k[5],k[6],k[7],k[0],k[1],k[2]]
        val.append(v1)
    df1 = pd.DataFrame(val,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term']).sort_values('Score',ascending=False)[['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term']].head(top)
    df = df.append(df1)

    ######### Adding log
    #dflog['first'] = f1_l
    #dflog['outset'] = out_set_l
    #dflog['len'] = len_l
    #dflog['code'] = code_l
    #dflog.to_csv('log-1.csv',index=False)
    
    #bar.finish()
  df = df.fillna("Not Applicable")
  df["Score"] = round(df['Score'] / df['Score'].max(),3)
  df["Score"] = df["Score"].astype('str')
  return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")

def feed(code,text):
  feedback.loc[feedback['Code']==code,'text'] = str(feedback[feedback['Code']==code]['text'].to_list()[0])+' '+str(text)
  return "Data Appended"

def feed_a(code,text,old_model,feed_added = 'feedback_model_a'):
  feedback = pd.read_pickle(old_model)
  feedback.loc[feedback['Code']==code,'text'] = str(feedback[feedback['Code']==code]['text'].to_list()[0])+' '+str(text)
  feedback.to_pickle(feed_added)
  return "Feedback Given to the Model and feedback model created by the name: "+str(feed_added)+' Dir: '+str(os.getcwd())

def feedback_clean(x):
  try:
    return ' '.join(list(set(x.split())))
  except:
    return x





#########################################################################################################################################################################################
#########################################################################################################################################################################################
#########################################################################   Annex - E.                   #########################################################
#########################################################################################################################################################################################
#########################################################################################################################################################################################

###### Rough
def exploding_feedback_data_E(data):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex E'] = f_file['IMDRF Annex E'].apply(expd)
  f_file = f_file.explode('IMDRF Annex E')

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex E'] = f_file['IMDRF Annex E'].apply(expd)
  f_file = f_file.explode('IMDRF Annex E')
  return f_file

ann_e = pd.read_csv('Annex_E.csv')

def oth_extraction(cn,foi):
  #other = ['E2401','E2402','E2403']
  other = [] #['E2403']
  o = []
  for i in other:

    v = [100,foi,i,ann_e[ann_e['Code']==i]['Definition'].tolist()[0]    ,ann_e[ann_e['Code']==i]['Level 1 Code'].tolist()[0],ann_e[ann_e['Code']==i]['Level 2 Code'].tolist()[0],ann_e[ann_e['Code']==i]['Level 3 Code'].tolist()[0],ann_e[ann_e['Code']==i]['Level 1 Term'].tolist()[0],ann_e[ann_e['Code']==i]['Level 2 Term'].tolist()[0],ann_e[ann_e['Code']==i]['Level 3 Term'].tolist()[0],ann_e[ann_e['Code']==i]['Non-IMDRF Code'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term','Non-IMDRF Code'])
  return d1

###################################################################     Predict
def prediction_E(inp,top,includeMeddra,model):
  fee = pd.read_pickle(model)
  #ann_e = pd.read_csv('Annex_E.csv')
  other = [] #['E2403']
  df = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term','Non-IMDRF Code'])
  df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term','Non-IMDRF Code'])  
  dflog = pd.DataFrame()  
  wordnet_lemmatizer = WordNetLemmatizer()
  val = []
  f1_l = []
  out_set_l = []
  len_l = []
  code_l = []

  for j in inp:   
    val = []
    v1 = []
    df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term','Non-IMDRF Code'])

    innn = purificationV1(j) +j.split()
    syn_inn = syn_list(innn)
    
    for k in ann_e.values:
      #wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      wd = str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      #wd = ' '.join(purificationV1(wd)) #test
      a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
      a = [re.sub('[0-9]', '', i) for i in a]
      a = [i for i in a if len(i) > 1]
      clean_word_1 = a
      """ # feedback and annexture syno
      clean_word_1 = []
      for i in a:
        clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
      clean_word_1 = list(set(clean_word_1))
      """

      #in_text = purificationV1(j) #+['empty'] #.split()   # Commented by Me-------->Aswin
      #clean_word_1 = purificationV1(' '.join(clean_word_1))

      ###### Adding Synonyms
      def_syn = clean_word_1   #list(set(syn_list(clean_word_1) + clean_word_1))    ## commented aswin for test
      #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin
      foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin#########################################-------> Aswin
      def_syn = [name.lower() for name in def_syn]
      foi_syn = [name.lower() for name in foi_syn]
 
      #out_set = set(clean_word_1) & set(in_text)            # commented by Me
      out_set = set(def_syn) & set(foi_syn)            # Added by Me
      f1 = out_set
      out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
      #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

      if len(out_set) >= 1:
        #print('Match')
        #print(in_text,"===",clean_word_1,"===",out_set)
        #print(clean_word_1)
        f1_l.append(f1)
        out_set_l.append(out_set)
        len_l.append(len(out_set))
        code_l.append(k[3])
        ############################# added for test log^
        
        #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
        lent = (len(out_set) / len(def_syn) ) *100
        #print(len(def_syn),'  ',k[3],'  ',in_text)

        """if lent < 30:
          ot_code = random.choice(other)
          v1 = [len(out_set),j,ot_code,k[4]]"""  # Commenting 31-01-2023 Jasmine stop 

        v1 = [len(out_set),j,k[3],k[4],k[5],k[6],k[7],k[0],k[1],k[2],k[8]]
        #print(lent,'  ',k[3])
        #v1 = [lent,j,k[3],k[4]]
        val.append(v1)
    df1 = pd.DataFrame(val,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term','Non-IMDRF Code'])
    #if df1.cn.max() < 30:
    dd1 = oth_extraction(df1.Score.max(),j)
    df1 = df1.append(dd1)

    df1 = df1.sort_values('Score',ascending=False)[['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term','Non-IMDRF Code']].head(top)
    #df1 = df1.sample(frac = 1)   # commented to stop rand Jasmine
    df = df.append(df1)



    ######### Adding log
    """dflog['first'] = f1_l
    dflog['outset'] = out_set_l
    dflog['len'] = len_l
    dflog['code'] = code_l
    dflog.to_csv('log-1.csv',index=False)"""


  if includeMeddra == 1:
      medra_list = []
      
      for i in df['Non-IMDRF Code'].values:
          
          try:
              code = i.split(":")[1]
              medra_list.append(meddra.meddraLookup(code))
          except:
              medra_list.append([{"Code":"Not Applicable"}])
              #medra_list.append([{"error":"Unable to complete request"}])
      df['Meddra'] = medra_list
      df = df.fillna("Not Applicable")
      df["Score"] = round(df['Score'] / df['Score'].max(),3)
      df["Score"] = df["Score"].astype('str')
      return df[["EventDescription","Definition","Meddra","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]]

  df = df.fillna("Not Applicable")
  df["Score"] = round(df['Score'] / df['Score'].max(),3)
  df["Score"] = df["Score"].astype('str')
  df = df.drop_duplicates(subset="code")### Code added for remove duplicates from E
  return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")



#########################################################################################################################################################################################
#########################################################################################################################################################################################
#########################################################################   Annex - B.                   #########################################################
#########################################################################################################################################################################################
#########################################################################################################################################################################################

ann_b = pd.read_csv('AnnexB.csv')

###### Rough
def exploding_feedback_data_B(data):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex B'] = f_file['IMDRF Annex B'].apply(expd)
  f_file = f_file.explode('IMDRF Annex B')

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex B'] = f_file['IMDRF Annex B'].apply(expd)
  f_file = f_file.explode('IMDRF Annex B')
  return f_file


def feed_b(code,text):
  fee_b.loc[fee_b['Code']==code,'text'] = str(fee_b[fee_b['Code']==code]['text'].to_list()[0])+' '+str(text)
  return "Data Appended"

def oth_extraction_b(cn,foi):
  other = []#['B22']
  o = []
  for i in other:
    v = [100,foi,i,ann_b[ann_b['Code']==i]['Definition'].tolist()[0]  ,ann_b[ann_b['Code']==i]['Level 1 Term'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','Level 1 Code','Definition','Level 1 Term'])
  return d1

def oth_extraction_fn_b(cn,foi):
  other = [cn]#['B22']
  o = []
  for i in other:
    v = [1,foi,i,ann_b[ann_b['Code']==i]['Definition'].tolist()[0]  ,ann_b[ann_b['Code']==i]['Level 1 Term'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','Level 1 Code','Definition','Level 1 Term'])
  return d1

###################################################################     Predict
def prediction_B(inp,top,itm_ret,model):
  if itm_ret != 1:
    fee_b = pd.read_pickle(model)
    df = pd.DataFrame(columns=['Score','EventDescription','Level 1 Code','Definition','Level 1 Term'])
    df1 = pd.DataFrame(columns=['Score','EventDescription','Level 1 Code','Definition','Level 1 Term'])  
    dflog = pd.DataFrame()  
    wordnet_lemmatizer = WordNetLemmatizer()
    val = []
    f1_l = []
    out_set_l = []
    len_l = []
    code_l = []

    for j in inp:    #['A lens was torn']:
      val = []
      v1 = []
      df1 = pd.DataFrame(columns=['Score','EventDescription','Level 1 Code','Definition','Level 1 Term'])

      innn = purificationV1(j) +j.split()   #-----> Performance Added by me
      syn_inn = syn_list(innn)     #-----> Performance Added by me
      
      for k in ann_b.values:
        #wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
        wd = str(fee_b[fee_b['Code']==k[1]]['text'].to_list()[0]).lower().replace('nan','')
        #wd = ' '.join(purificationV1(wd)) #test
        a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
        a = [re.sub('[0-9]', '', i) for i in a]
        a = [i for i in a if len(i) > 1]
        clean_word_1 = a
        """ # feedback and annexture syno
        clean_word_1 = []
        for i in a:
          clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
        clean_word_1 = list(set(clean_word_1))
        """

        #in_text = purificationV1(j) #+['empty'] #.split()   # Commented by Me
        #clean_word_1 = purificationV1(' '.join(clean_word_1))

        ###### Adding Synonyms
        def_syn = clean_word_1   #list(set(syn_list(clean_word_1) + clean_word_1))    ## commented aswin for test
        #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin---------> Aswin
        foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin#########################################-------> Aswin
        def_syn = [name.lower() for name in def_syn]
        foi_syn = [name.lower() for name in foi_syn]
  
        #out_set = set(clean_word_1) & set(in_text)            # commented by Me
        out_set = set(def_syn) & set(foi_syn)            # Added by Me
        f1 = out_set
        out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
        #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

        if len(out_set) >= 1:
          #print('Match')
          #print(in_text,"===",clean_word_1,"===",out_set)
          #print(clean_word_1)
          f1_l.append(f1)
          out_set_l.append(out_set)
          len_l.append(len(out_set))
          code_l.append(k[1])
          ############################# added for test log^
          
          #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
          lent = (len(out_set) / len(def_syn) ) *100
          #print(len(def_syn),'  ',k[3],'  ',in_text)

          """if lent < 30:
            ot_code = random.choice(other)
            v1 = [len(out_set),j,ot_code,k[4]]"""

          v1 = [len(out_set),j,k[1],k[2],k[0]]
          #print(lent,'  ',k[3])
          #v1 = [lent,j,k[3],k[4]]
          val.append(v1)
      df1 = pd.DataFrame(val,columns=['Score','EventDescription','Level 1 Code','Definition','Level 1 Term'])
      #if df1.cn.max() < 30:
      dd1 = oth_extraction_b(df1.Score.max(),j)
      df1 = df1.append(dd1)

      df1 = df1.sort_values('Score',ascending=False)[['Score','EventDescription','Level 1 Code','Definition','Level 1 Term']].head(top)
      #df1 = df1.sample(frac = 1) # commented to stop random gen jasmine
      df = df.append(df1)



      ######### Adding log
      """dflog['first'] = f1_l
      dflog['outset'] = out_set_l
      dflog['len'] = len_l
      dflog['code'] = code_l
      dflog.to_csv('log-1.csv',index=False)"""

    df = df.fillna("Not Applicable")
    df["Score"] = round(df['Score'] / df['Score'].max(),3)
    df["Score"] = df["Score"].astype('str')
    df[["Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]] = "Not Applicable"     ## Added by Jasmine as per Client Request
    #df = df.drop_duplicates(subset="code")
    #return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code"]]
    df = df.drop_duplicates(subset="Level 1 Code")### Code added for remove duplicates from C added on 24-01-2023
    return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","") ## Added by Jasmine as per Client Request
  else:
    print("test --------")
    out = oth_extraction_fn_b('B01',inp[0])
    out[["Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]] = "" 
    out = out.fillna('')
    return out[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","") ## Added by Jasmine as per Client Request

  #return df





#########################################################################################################################################################################################
#########################################################################################################################################################################################
#########################################################################   Annex - C.                   #########################################################
#########################################################################################################################################################################################
#########################################################################################################################################################################################

#ac = pd.read_excel("annex bcdf.xlsx")
ann_c = pd.read_csv('AnnexC.csv')
fee_c = pd.read_pickle("model_CV1")

###### Rough
def exploding_feedback_data_C(data):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex C'] = f_file['IMDRF Annex C'].apply(expd)
  f_file = f_file.explode('IMDRF Annex C')

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex C'] = f_file['IMDRF Annex C'].apply(expd)
  f_file = f_file.explode('IMDRF Annex C')
  return f_file

def feed_c(code,text):
  fee_c.loc[fee_c['Code']==code,'text'] = str(fee_c[fee_c['Code']==code]['text'].to_list()[0])+' '+str(text)
  return "Data Appended"

def oth_extraction_c(cn,foi):
  other = []
  o = []
  for i in other:
    v = [100,foi,i,ann_c[ann_c['Code']==i]['Definition'].tolist()[0]    ,ann_c[ann_c['Code']==i]['Level 1 Code'].tolist()[0],ann_c[ann_c['Code']==i]['Level 2 Code'].tolist()[0],ann_c[ann_c['Code']==i]['Level 3 Code'].tolist()[0],ann_c[ann_c['Code']==i]['Level 1 Term'].tolist()[0],ann_c[ann_c['Code']==i]['Level 2 Term'].tolist()[0],ann_c[ann_c['Code']==i]['Level 3 Term'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  return d1

###################################################################     Predict
def prediction_C(inp,top,model):
  fee_c = pd.read_pickle(model)
  df = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])  
  dflog = pd.DataFrame()  
  wordnet_lemmatizer = WordNetLemmatizer()
  val = []
  f1_l = []
  out_set_l = []
  len_l = []
  code_l = []

  for j in inp:    #['A lens was torn']:
    val = []
    v1 = []
    df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])

    innn = purificationV1(j) +j.split()#------> Jasmine Added for performance tuning
    syn_inn = syn_list(innn)#------> Jasmine Added for performance tuning
    
    for k in ann_c.values:
      #wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      wd = str(fee_c[fee_c['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      #wd = ' '.join(purificationV1(wd)) #test
      a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
      a = [re.sub('[0-9]', '', i) for i in a]
      a = [i for i in a if len(i) > 1]
      clean_word_1 = a
      """ # feedback and annexture syno
      clean_word_1 = []
      for i in a:
        clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
      clean_word_1 = list(set(clean_word_1))
      """

      in_text = purificationV1(j) #+['empty'] #.split()   # Commented by Me
      #clean_word_1 = purificationV1(' '.join(clean_word_1))

      ###### Adding Synonyms
      def_syn = clean_word_1   #list(set(syn_list(clean_word_1) + clean_word_1))    ## commented aswin for test
      #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin#------> Jasmine Commented for performance tuning
      foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin##########################################------> Jasmine Added for performance tuning
      def_syn = [name.lower() for name in def_syn]
      foi_syn = [name.lower() for name in foi_syn]
 
      #out_set = set(clean_word_1) & set(in_text)            # commented by Me
      out_set = set(def_syn) & set(foi_syn)            # Added by Me
      f1 = out_set
      out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
      #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

      if len(out_set) >= 1:
        #print('Match')
        #print(in_text,"===",clean_word_1,"===",out_set)
        #print(clean_word_1)
        f1_l.append(f1)
        out_set_l.append(out_set)
        len_l.append(len(out_set))
        code_l.append(k[3])
        ############################# added for test log^
        
        #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
        lent = (len(out_set) / len(def_syn) ) *100
        #print(len(def_syn),'  ',k[3],'  ',in_text)

        """if lent < 30:
          ot_code = random.choice(other)
          v1 = [len(out_set),j,ot_code,k[4]]"""

        v1 = [len(out_set),j,k[3],k[4],k[5],k[6],k[7],k[0],k[1],k[2]]
        #print(lent,'  ',k[3])
        #v1 = [lent,j,k[3],k[4]]
        val.append(v1)
    df1 = pd.DataFrame(val,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
    #if df1.cn.max() < 30:
    dd1 = oth_extraction_c(df1.Score.max(),j)
    df1 = df1.append(dd1)

    df1 = df1.sort_values('Score',ascending=False)[['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term']].head(top)
    #df1 = df1.sample(frac = 1) # commented to stop random gen jasmine
    df = df.append(df1)



    ######### Adding log
    """dflog['first'] = f1_l
    dflog['outset'] = out_set_l
    dflog['len'] = len_l
    dflog['code'] = code_l
    dflog.to_csv('log-1.csv',index=False)"""

  df = df.fillna('Not Applicable')
  df["Score"] = round(df['Score'] / df['Score'].max(),3)
  df["Score"] = df["Score"].astype('str')

  df = df.drop_duplicates(subset="code")### Code added for remove duplicates from E

  return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")







#########################################################################################################################################################################################
#########################################################################################################################################################################################
#########################################################################   Annex - D.                   #########################################################
#########################################################################################################################################################################################
#########################################################################################################################################################################################

ann_d = pd.read_csv('AnnexD.csv')
fee_d = pd.read_pickle("model_DV1")

###### Rough
def exploding_feedback_data_D(data):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex D'] = f_file['IMDRF Annex D'].apply(expd)
  f_file = f_file.explode('IMDRF Annex D')

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex D'] = f_file['IMDRF Annex D'].apply(expd)
  f_file = f_file.explode('IMDRF Annex D')
  return f_file

def feed_d(code,text):
  fee_d.loc[fee_d['Code']==code,'text'] = str(fee_d[fee_d['Code']==code]['text'].to_list()[0])+' '+str(text)
  return "Data Appended"

def oth_extraction_d(cn,foi):
  other = [] #['D16']
  o = []
  for i in other:
    v = [100,foi,i,ann_d[ann_d['Code']==i]['Definition'].tolist()[0]   ,ann_d[ann_d['Code']==i]['Level 1 Term'].tolist()[0],ann_d[ann_d['Code']==i]['Level 2 Term'].tolist()[0],ann_d[ann_d['Code']==i]['Level 1 Code'].tolist()[0],ann_d[ann_d['Code']==i]['Level 2 Code'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','code','Definition','Level 1 Term','Level 2 Term','Level 1 Code','Level 2 Code'])
  return d1

###################################################################     Predict
def prediction_D(inp,top,model):
  fee_d = pd.read_pickle(model)
  df = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Term','Level 2 Term','Level 1 Code','Level 2 Code'])
  df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Term','Level 2 Term','Level 1 Code','Level 2 Code'])  
  dflog = pd.DataFrame()  
  wordnet_lemmatizer = WordNetLemmatizer()
  val = []
  f1_l = []
  out_set_l = []
  len_l = []
  code_l = []

  for j in inp:    #['A lens was torn']:
    val = []
    v1 = []
    df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Term','Level 2 Term','Level 1 Code','Level 2 Code'])

    innn = purificationV1(j) +j.split()#------> Jasmine Jeniffer J Added for performance tuning
    syn_inn = syn_list(innn)#------> Jasmine Added for performance tuning
    
    for k in ann_d.values:
      #wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      wd = str(fee_d[fee_d['Code']==k[2]]['text'].to_list()[0]).lower().replace('nan','')
      #wd = ' '.join(purificationV1(wd)) #test
      a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
      a = [re.sub('[0-9]', '', i) for i in a]
      a = [i for i in a if len(i) > 1]
      clean_word_1 = a
      """ # feedback and annexture syno
      clean_word_1 = []
      for i in a:
        clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
      clean_word_1 = list(set(clean_word_1))
      """

      #in_text = purificationV1(j) #+['empty'] #.split()   # Commented by Me
      #clean_word_1 = purificationV1(' '.join(clean_word_1))

      ###### Adding Synonyms
      def_syn = clean_word_1   #list(set(syn_list(clean_word_1) + clean_word_1))    ## commented aswin for test
      #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin #------> Jasmine Added for performance tuning
      foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin##########################################------> Jasmine Added for performance tuning
      def_syn = [name.lower() for name in def_syn]
      foi_syn = [name.lower() for name in foi_syn]
 
      #out_set = set(clean_word_1) & set(in_text)            # commented by Me
      out_set = set(def_syn) & set(foi_syn)            # Added by Me
      f1 = out_set
      out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
      #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

      if len(out_set) >= 1:
        #print('Match')
        #print(in_text,"===",clean_word_1,"===",out_set)
        #print(clean_word_1)
        f1_l.append(f1)
        out_set_l.append(out_set)
        len_l.append(len(out_set))
        code_l.append(k[2])
        ############################# added for test log^
        
        #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
        lent = (len(out_set) / len(def_syn) ) *100
        #print(len(def_syn),'  ',k[3],'  ',in_text)

        """if lent < 30:
          ot_code = random.choice(other)
          v1 = [len(out_set),j,ot_code,k[4]]"""

        v1 = [len(out_set),j,k[2],k[3],k[0],k[1],k[4],k[5]]
        #print(lent,'  ',k[3])
        #v1 = [lent,j,k[3],k[4]]
        val.append(v1)
    df1 = pd.DataFrame(val,columns=['Score','EventDescription','code','Definition','Level 1 Term','Level 2 Term','Level 1 Code','Level 2 Code'])
    #if df1.cn.max() < 30:
    dd1 = oth_extraction_d(df1.Score.max(),j)
    df1 = df1.append(dd1)

    df1 = df1.sort_values('Score',ascending=False)[['Score','EventDescription','code','Definition','Level 1 Term','Level 2 Term','Level 1 Code','Level 2 Code']].head(top)
    #df1 = df1.sample(frac = 1) # commented to stop random gen jasmine
    df = df.append(df1)



    ######### Adding log
    """dflog['first'] = f1_l
    dflog['outset'] = out_set_l
    dflog['len'] = len_l
    dflog['code'] = code_l
    dflog.to_csv('log-1.csv',index=False)"""

  df = df.fillna("Not Applicable")  
  df["Score"] = round(df['Score'] / df['Score'].max(),3)
  df["Score"] = df["Score"].astype('str')
  df[["Level 3 Term","Level 3 Code"]] = "Not Applicable"    # Jasmine added as per client request 

  df = df.drop_duplicates(subset="code")### Code added for remove duplicates from E
  
  return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")   # Jasmine added as per client request 









#########################################################################################################################################################################################
#########################################################################################################################################################################################
#########################################################################   Annex - F.                   #########################################################
#########################################################################################################################################################################################
#########################################################################################################################################################################################

ann_f = pd.read_csv('AnnexF.csv')
fee_f = pd.read_pickle("model_Fv1")

###### Rough
def exploding_feedback_data_F(data):
  f_file = data#pd.read_excel(data)
  #f_file[['Complaint_Description','IMDRF Annex A','FEEDBACK_CONTENT']]
  #len(f_file['Complaint_Description'].values.tolist())


  def expd(x):
    x = str(x).replace(' ','').split(',')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex F'] = f_file['IMDRF Annex F'].apply(expd)
  f_file = f_file.explode('IMDRF Annex F')

  def expd(x):
    x = str(x).replace(' ','').split(';')
    x = [item for item in x if item != '']
    return x #"["+str(x)+']'
  f_file['IMDRF Annex F'] = f_file['IMDRF Annex F'].apply(expd)
  f_file = f_file.explode('IMDRF Annex F')
  return f_file

def feed_d(code,text):
  fee_f.loc[fee_f['Code']==code,'text'] = str(fee_f[fee_f['Code']==code]['text'].to_list()[0])+' '+str(text)
  return "Data Appended"

def oth_extraction_f(cn,foi):
  other = [] #['F26','F11'] # no default jasmine
  o = []
  for i in other:
    v = [100,foi,i,ann_f[ann_f['Code']==i]['Definition'].tolist()[0]    ,ann_f[ann_f['Code']==i]['Level 1 Code'].tolist()[0],ann_f[ann_f['Code']==i]['Level 2 Code'].tolist()[0],ann_f[ann_f['Code']==i]['Level 3 Code'].tolist()[0],ann_f[ann_f['Code']==i]['Level 1 Term'].tolist()[0],ann_f[ann_f['Code']==i]['Level 2 Term'].tolist()[0],ann_f[ann_f['Code']==i]['Level 3 Term'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  return d1

###################################################################     Predict
def prediction_F(inp,top,model):
  fee_d = pd.read_pickle(model)
  df = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])  
  dflog = pd.DataFrame()  
  wordnet_lemmatizer = WordNetLemmatizer()
  val = []
  f1_l = []
  out_set_l = []
  len_l = []
  code_l = []

  for j in inp:    #['A lens was torn']:
    val = []
    v1 = []
    df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])

    innn = purificationV1(j) +j.split()#------> Jasmine Added for performance tuning
    syn_inn = syn_list(innn)#------> Jasmine Added for performance tuning
    
    for k in ann_f.values:
      #wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      wd = str(fee_f[fee_f['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
      #wd = ' '.join(purificationV1(wd)) #test
      a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
      a = [re.sub('[0-9]', '', i) for i in a]
      a = [i for i in a if len(i) > 1]
      clean_word_1 = a
      """ # feedback and annexture syno
      clean_word_1 = []
      for i in a:
        clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
      clean_word_1 = list(set(clean_word_1))
      """

      in_text = purificationV1(j) #+['empty'] #.split()   # Commented by Me
      #clean_word_1 = purificationV1(' '.join(clean_word_1))

      ###### Adding Synonyms
      def_syn = clean_word_1   #list(set(syn_list(clean_word_1) + clean_word_1))    ## commented aswin for test
      #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin#------> Jasmine Commented for performance tuning
      foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin##########################################------> Jasmine Added for performance tuning
      def_syn = [name.lower() for name in def_syn]
      foi_syn = [name.lower() for name in foi_syn]
 
      #out_set = set(clean_word_1) & set(in_text)            # commented by Me
      out_set = set(def_syn) & set(foi_syn)            # Added by Me
      f1 = out_set
      out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
      #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

      if len(out_set) >= 1:
        #print('Match')
        #print(in_text,"===",clean_word_1,"===",out_set)
        #print(clean_word_1)
        f1_l.append(f1)
        out_set_l.append(out_set)
        len_l.append(len(out_set))
        code_l.append(k[2])
        ############################# added for test log^
        
        #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
        lent = (len(out_set) / len(def_syn) ) *100
        #print(len(def_syn),'  ',k[3],'  ',in_text)

        """if lent < 30:
          ot_code = random.choice(other)
          v1 = [len(out_set),j,ot_code,k[4]]"""

        v1 = [len(out_set),j,k[3],k[4],k[5],k[6],k[7],k[0],k[1],k[2]]
        #print(lent,'  ',k[3])
        #v1 = [lent,j,k[3],k[4]]
        val.append(v1)
    df1 = pd.DataFrame(val,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
    #if df1.cn.max() < 30:
    dd1 = oth_extraction_f(df1.Score.max(),j)
    df1 = df1.append(dd1)

    df1 = df1.sort_values('Score',ascending=False)[['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term']].head(top)
    #df1 = df1.sample(frac = 1) # commented to stop random gen jasmine
    df = df.append(df1)



    ######### Adding log
    """dflog['first'] = f1_l
    dflog['outset'] = out_set_l
    dflog['len'] = len_l
    dflog['code'] = code_l
    dflog.to_csv('log-1.csv',index=False)"""
  df = df.fillna('Not Applicable')
  df["Score"] = round(df['Score'] / df['Score'].max(),3)
  df["Score"] = df["Score"].astype('str')
  df = df.drop_duplicates(subset="code")
  return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")





#########################################################################################################################################################################################
#########################################################################################################################################################################################
#########################################################################   Annex - G.                   #########################################################
#########################################################################################################################################################################################
#########################################################################################################################################################################################

#ac = pd.read_excel("annex bcdf.xlsx")
ann_g = pd.read_csv('Annex_G.csv')
fee_g = pd.read_pickle("model_Gv1")



def feed_c(code,text):
  fee_g.loc[fee_g['Code']==code,'text'] = str(fee_g[fee_g['Code']==code]['text'].to_list()[0])+' '+str(text)
  return "Data Appended"

def oth_extraction_g(cn,foi):
  other = []
  o = []
  for i in other:
    v = [100,foi,i,ann_g[ann_g['Code']==i]['Definition'].tolist()[0]    ,ann_g[ann_g['Code']==i]['Level 1 Code'].tolist()[0],ann_g[ann_g['Code']==i]['Level 2 Code'].tolist()[0],ann_g[ann_g['Code']==i]['Level 3 Code'].tolist()[0],ann_g[ann_g['Code']==i]['Level 1 Term'].tolist()[0],ann_g[ann_g['Code']==i]['Level 2 Term'].tolist()[0],ann_g[ann_g['Code']==i]['Level 3 Term'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  return d1

def item_returned_fn_g(cn,foi):
  other = [cn]
  o = []
  for i in other:
    v = [1,foi,i,ann_g[ann_g['Code']==i]['Definition'].tolist()[0]    ,ann_g[ann_g['Code']==i]['Level 1 Code'].tolist()[0],ann_g[ann_g['Code']==i]['Level 2 Code'].tolist()[0],ann_g[ann_g['Code']==i]['Level 3 Code'].tolist()[0],ann_g[ann_g['Code']==i]['Level 1 Term'].tolist()[0],ann_g[ann_g['Code']==i]['Level 2 Term'].tolist()[0],ann_g[ann_g['Code']==i]['Level 3 Term'].tolist()[0]]
    o.append(v)
  d1 = pd.DataFrame(o,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
  return d1

###################################################################     Predict
def prediction_G(inp,top,itm_ret,model):
  if itm_ret != 1:
    fee_g = pd.read_pickle(model)
    df = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
    df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])  
    dflog = pd.DataFrame()  
    wordnet_lemmatizer = WordNetLemmatizer()
    val = []
    f1_l = []
    out_set_l = []
    len_l = []
    code_l = []

    for j in inp:    #['A lens was torn']:
      val = []
      v1 = []
      df1 = pd.DataFrame(columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])

      innn = purificationV1(j) +j.split()#------> Jasmine Added for performance tuning
      syn_inn = syn_list(innn)#------> Jasmine Added for performance tuning
      
      for k in ann_g.values:
        #wd = str(k[0])+' '+str(k[1])+' '+str(k[2])+' '+str(k[4])+' '+ str(fee[fee['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
        wd = str(fee_g[fee_g['Code']==k[3]]['text'].to_list()[0]).lower().replace('nan','')
        #wd = ' '.join(purificationV1(wd)) #test
        a = [re.sub('[^A-Za-z0-9]+', '', i) for i in wd.split()]
        a = [re.sub('[0-9]', '', i) for i in a]
        a = [i for i in a if len(i) > 1]
        clean_word_1 = a
        """ # feedback and annexture syno
        clean_word_1 = []
        for i in a:
          clean_word_1.append(wordnet_lemmatizer.lemmatize(i.lower(), pos="v"))
        clean_word_1 = list(set(clean_word_1))
        """

        in_text = purificationV1(j) #+['empty'] #.split()   # Commented by Me
        syn_inn = syn_list(in_text)
        #clean_word_1 = purificationV1(' '.join(clean_word_1))

        ###### Adding Synonyms
        def_syn = clean_word_1   #list(set(syn_list(clean_word_1) + clean_word_1))    ## commented aswin for test
        #foi_syn = list(set(syn_list(in_text) + in_text))#in_text     #test commented aswin#------> Jasmine Commented for performance tuning
        foi_syn = list(set(syn_inn + innn))#in_text     #test commented aswin##########################################------> Jasmine Added for performance tuning
        def_syn = [name.lower() for name in def_syn]
        foi_syn = [name.lower() for name in foi_syn]
  
        #out_set = set(clean_word_1) & set(in_text)            # commented by Me
        out_set = set(def_syn) & set(foi_syn)            # Added by Me
        f1 = out_set
        out_set = out_set & set(def_syn) ##### Added by aswin for further filtering
        #print("match_counts: ",k[3],'==',len(out_set),foi_syn)

        if len(out_set) >= 1:
          #print('Match')
          #print(in_text,"===",clean_word_1,"===",out_set)
          #print(clean_word_1)
          f1_l.append(f1)
          out_set_l.append(out_set)
          len_l.append(len(out_set))
          code_l.append(k[3])
          ############################# added for test log^
          
          #print(len(out_set),' ',"code: ",k[3],'<==>',k[4])
          lent = (len(out_set) / len(def_syn) ) *100
          #print(len(def_syn),'  ',k[3],'  ',in_text)

          """if lent < 30:
            ot_code = random.choice(other)
            v1 = [len(out_set),j,ot_code,k[4]]"""

          v1 = [len(out_set),j,k[3],k[4],k[5],k[6],k[7],k[0],k[1],k[2]]
          #print(lent,'  ',k[3])
          #v1 = [lent,j,k[3],k[4]]
          val.append(v1)
      df1 = pd.DataFrame(val,columns=['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term'])
      #if df1.cn.max() < 30:
      dd1 = oth_extraction_g(df1.Score.max(),j)
      df1 = df1.append(dd1)

      df1 = df1.sort_values('Score',ascending=False)[['Score','EventDescription','code','Definition','Level 1 Code','Level 2 Code','Level 3 Code','Level 1 Term', 'Level 2 Term', 'Level 3 Term']].head(top)
      #df1 = df1.sample(frac = 1) # commented to stop random gen jasmine
      df = df.append(df1)


    df = df.fillna('Not Applicable')
    df["Score"] = round(df['Score'] / df['Score'].max(),3)
    df["Score"] = df["Score"].astype('str')

    df = df.drop_duplicates(subset="code")### Code added for remove duplicates from E

    return df[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")
  else:
    print("test test==",inp[0])
    out = item_returned_fn_g('G07001',inp[0])
    out = out.fillna('')
    return out[["EventDescription","Definition","Score","Level 1 Term","Level 1 Code","Level 2 Term","Level 2 Code","Level 3 Term","Level 3 Code"]].replace("Not Applicable","").replace("not applicable","")

